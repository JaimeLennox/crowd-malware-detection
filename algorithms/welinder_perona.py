import functools
import numpy as np
from scipy.stats import norm

from algorithms import threshold
from utils import data
from utils.parallel import parmap


class WelinderPerona:

    def __init__(self, engine_count, data_dict, type_dict, train_test=False):
        self._engine_count = engine_count
        self._data_dict = data_dict
        self._type_dict = type_dict

        self._train_test = train_test
        self._training_proportion = 0.8
        self._repeats = 10 if train_test else 1

    def run(self):
        print "Welinder Perona Method:"

        comp_results = []

        for i in range(self._repeats):
            if self._train_test:
                self._train_data, self._test_data = data.train_test_data(self._data_dict, self._training_proportion)
            else:
                self._train_data = self._data_dict
                self._test_data = self._data_dict

            results, sensitivity, specificity, experts, bots, all_annotators  = self._run()
            annotators = experts if experts else [i for i in all_annotators if i not in bots]
            test_results = self._test_results(results, sensitivity, specificity, annotators)
            comp_results.append(data.calculate_results(test_results, self._type_dict))

        self._print_comp_results(comp_results)

    def _run(self):
        # Initialisation
        M = self._engine_count

        all_annotators = range(M)
        experts = set()
        bot = set()

        variance_threshold = 0.001
        gamma_threshold = 0.5

        a1 = 3
        a2 = 2
        b1 = 3
        b2 = 2

        sensitivity = np.zeros(M)
        specificity = np.zeros(M)

        mu = threshold.Threshold(self._train_data, self._type_dict).majority_vote()

        mu_diff = 1

        while mu_diff > 0:

            def calculate_mu_sums(j, self):
                sum_mu = 0
                sum_neg_mu = 0
                sum_mu_y = 0
                sum_neg_mu_y = 0

                for k, v in mu.iteritems():
                    if self._data_dict[k][j] < 2:
                        sum_mu += v
                        sum_neg_mu += 1 - v
                        sum_mu_y += v * self._data_dict[k][j]
                        sum_neg_mu_y += (1 - v) * (1 - self._data_dict[k][j])

                return j, sum_mu, sum_neg_mu, sum_mu_y, sum_neg_mu_y

            annotators = experts if experts else [i for i in all_annotators if i not in bot]
            results = parmap(functools.partial(calculate_mu_sums, self=self), annotators)

            sum_mu = np.zeros(M)
            sum_neg_mu = np.zeros(M)
            sum_mu_y = np.zeros(M)
            sum_neg_mu_y = np.zeros(M)

            for j, sum_mu_j, sum_neg_mu_j, sum_mu_y_j, sum_neg_mu_y_j in results:
                sum_mu[j] = sum_mu_j
                sum_neg_mu[j] = sum_neg_mu_j
                sum_mu_y[j] = sum_mu_y_j
                sum_neg_mu_y[j] = sum_neg_mu_y_j

            experts = set()

            sensitivity = np.zeros(M)
            specificity = np.zeros(M)

            for annotator in annotators:
                a_alpha = a1 - 1 + sum_mu_y[annotator]
                a_beta = a1 + a2 - 2 + sum_mu[annotator] - a_alpha

                b_alpha = b1 - 1 + sum_neg_mu_y[annotator]
                b_beta = b1 + b2 - 2 + sum_neg_mu[annotator] - b_alpha

                sensitivity[annotator] = a_alpha / float(a_alpha + a_beta)
                specificity[annotator] = b_alpha / float(b_alpha + b_beta)

                a_var = (a_alpha * a_beta) / float(((a_alpha + a_beta) ** 2) * (a_alpha + a_beta + 1))
                b_var = (b_alpha * b_beta) / float(((b_alpha + b_beta) ** 2) * (b_alpha + b_beta + 1))

                if (a_var + b_var) < variance_threshold:

                    # print (a_var + b_var)

                    difference = norm.ppf(specificity[annotator]) - norm.ppf(1 - sensitivity[annotator])

                    if difference > 2:
                        experts.add(annotator)
                    else:
                        bot.add(annotator)

            prevalence = self._calculate_prevalence(mu)

            def calculate_mu(iv):
                i, v = iv

                mu_prob = self._calculate_posterior(v, sensitivity, specificity, annotators, prevalence)
                mu_result = 1 if mu_prob > gamma_threshold else 0

                diff = abs(mu[i] - mu_result)
                return i, mu_result, diff

            results = parmap(calculate_mu, self._train_data.iteritems())

            mu_diff = 0

            for i, mu_result, diff in results:
                mu[i] = mu_result
                mu_diff += diff

            accuracy = data.correct_count(mu, self._type_dict) / float(len(mu))
            print accuracy

        print "Experts:", experts
        print "Bot:", bot
        print "Leftovers", [i for i in all_annotators if i not in experts and i not in bot]

        return mu, sensitivity, specificity, experts, bot, all_annotators

    def _calculate_prevalence(self, mu):
        p1 = 2
        p2 = 2
        sum_mu = sum([v for v in mu.values()])
        return (p1 - 1 + sum_mu) / float(p1 + p2 - 2 + len(mu))

    def _calculate_posterior(self, data, sensitivity, specificity, annotators, prevalence):
        indices = [index for index, x in enumerate(data) if x < 2 and index in annotators]

        ys = np.array(data)[indices]
        sensitivities = sensitivity[indices]
        specificities = specificity[indices]
        neg_ys = 1 - ys

        a = np.exp(np.sum(np.log((sensitivities ** ys) * ((1 - sensitivities) ** neg_ys))))
        b = np.exp(np.sum(np.log((specificities ** neg_ys) * ((1 - specificities) ** ys))))

        mu_result = (a * prevalence) / (a * prevalence + (b * (1 - prevalence)))
        return mu_result

    def _test_results(self, results, sensitivity, specificity, A):
        prevalence = self._calculate_prevalence(results)
        return {k: 1 if self._calculate_posterior(v, sensitivity, specificity, A, prevalence) > 0.5 else 0 for k, v in self._test_data.iteritems()}

    def _print_comp_results(self, comp_results):
        avg_error, avg_fpr, avg_fnr = map(lambda x: x / float(len(comp_results)), map(sum, zip(*comp_results)))

        print "Average Error Rate", avg_error
        print "Average False Positive Rate", avg_fpr
        print "Average False Negative Rate", avg_fnr






























