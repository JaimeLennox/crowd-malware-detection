import numpy as np

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve

eps = 0.0001


def create_data_dict(data_file):
    with open(data_file) as file:
        lines = [line.rstrip(';\r\n') for line in file]

    data_list = map(lambda x: x.split(';'), lines)
    data_dict = {}

    for data in data_list:
        data_dict[data[0]] = map(int, data[1:])

    return data_dict


def count_antivirus_engines(engine_file):
    count = 0

    with open(engine_file) as engines:
        for line in engines:
            if line.strip():
                count += 1

    return count


def get_malware_proportions(result_values):
    return result_values.count(0), result_values.count(1)


def sensitivity_graph(learned_sensitivity, true_sensitivity):
    plt.xlim(0, 1)
    plt.ylim(0, 1)

    plt.xlabel("True Sensitivity")
    plt.ylabel("Learned Sensitivity")

    # Colour abnormally different results red, else blue
    colours = ['r' if abs(learned_sensitivity[i] - true_sensitivity[i]) > 0.5 else 'b' for i in range(len(true_sensitivity))]

    plt.scatter(true_sensitivity, learned_sensitivity, c=colours)
    plt.plot((0, 1), 'r-')
    plt.tight_layout()
    plt.show()


def specificity_graph(learned_specificity, true_specificity):
    plt.xlim(0, 1)
    plt.ylim(0, 1)

    plt.xlabel("True Specificity")
    plt.ylabel("Learned Specificity")

    colours = ['r' if abs(learned_specificity[i] - true_specificity[i]) > 0.5 else 'b' for i in range(len(true_specificity))]

    plt.scatter(true_specificity, learned_specificity, c=colours)
    plt.plot((0, 1), 'r-')
    plt.tight_layout()
    plt.show()


def supervised_graph_accuracy(supervised_proportion, accuracy):
    plt.xlim(0, 1)
    plt.ylim(min(accuracy) - 0.01, max(accuracy) + 0.01)

    plt.xlabel("Supervised Proportion")
    plt.ylabel("Accuracy")

    plt.scatter(supervised_proportion, accuracy)
    plt.plot(supervised_proportion, accuracy)

    plt.tight_layout()
    plt.show()


def supervised_graph(supervised_proportion, sensitivity_diff, specificity_diff):
    plt.xlim(0, 1)

    plt.xlabel("Supervised Proportion")
    plt.ylabel("Average Absolute Error")

    scatters = []
    plots = []

    scatters.append(plt.scatter(supervised_proportion, sensitivity_diff, color='r'))
    plots.append(plt.plot(supervised_proportion, sensitivity_diff, color='r', label='Sensitivity'))

    scatters.append(plt.scatter(supervised_proportion, specificity_diff, color='b'))
    plots.append(plt.plot(supervised_proportion, specificity_diff, color='b', label='Specificity', linestyle='--'))

    plt.legend()

    plt.tight_layout()
    plt.show()


def roc_graph(fpr, tpr, thresholds):
    colours = iter(['r', 'b', 'g'])

    scatters = [plt.scatter(fpr, tpr, color=next(colours))]

    plt.legend(scatters, ("Threshold", "Raykar"))

    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")

    plt.xlim(10 ** -3, 10 ** -2)
    plt.xscale('log')

    plt.ylim(0.5, 1)

    plt.tight_layout()

    plt.show()


def annotator_graph(accuracies, engine_count):
    plt.xlim(1, engine_count)

    plt.xlabel("Number of (Best) Engines")
    plt.ylabel("Accuracy")

    plt.scatter(range(1, engine_count + 1), accuracies)
    plt.plot(range(1, engine_count + 1), accuracies)

    plt.tight_layout()

    plt.show()


def annotator_graph_fpr(fpr, engine_count):
    plt.xlim(1, engine_count)

    plt.xlabel("Number of (Best) Engines")
    plt.ylabel("False Positive Rate")

    plt.scatter(range(1, engine_count + 1), fpr)
    plt.plot(range(1, engine_count + 1), fpr)

    plt.tight_layout()

    plt.show()


def annotator_model(engine_count, data_dict, type_dict):
    sensitivity = np.zeros(engine_count)
    specificity = np.zeros(engine_count)
    malware_count = np.zeros(engine_count)
    goodware_count = np.zeros(engine_count)

    for k, v in data_dict.iteritems():
        for i in range(engine_count):
            if v[i] < 2:
                sensitivity[i] += 1 if v[i] == 1 and type_dict[k] == 1 else 0
                specificity[i] += 1 if v[i] == 0 and type_dict[k] == 0 else 0

                if type_dict[k] == 1:
                    malware_count[i] += 1
                else:
                    goodware_count[i] += 1

    sensitivity = [sensitivity[i] / malware_count[i] if malware_count[i] != 0 else 0 for i in range(len(sensitivity))]
    specificity = [specificity[i] / goodware_count[i] if goodware_count[i] != 0 else 0 for i in range(len(specificity))]

    return sensitivity, specificity


def correct_count(results, type_dict):
    return sum([1 if results[result] == type_dict[result] else 0 for result in results])


def error_rate(results, type_dict):
    return 1 - (correct_count(results, type_dict) / float(len(results)))


def fpr(results, type_dict):
    return sum([1 if results[result] == 1 and type_dict[result] == 0 else 0 for result in results]) / float(type_dict.values().count(0) + eps)


def fnr(results, type_dict):
    return 1 - (sum([1 if results[result] == 1 and type_dict[result] == 1 else 0 for result in results]) / float(type_dict.values().count(1) + eps))


def calculate_results(results, type_dict):
    p = 0.0
    n = 0.0

    tp = 0.0
    tn = 0.0
    fp = 0.0
    fn = 0.0

    for result in results:
        p += type_dict[result]
        n += 1 - type_dict[result]

        tp += 1 if results[result] == 1 and type_dict[result] == 1 else 0
        tn += 1 if results[result] == 0 and type_dict[result] == 0 else 0
        fp += 1 if results[result] == 1 and type_dict[result] == 0 else 0
        fn += 1 if results[result] == 0 and type_dict[result] == 1 else 0

    tpr = tp / p
    tnr = tn / n
    fpr = 1 - tnr
    fnr = 1 - tpr

    error = 1 - ((tp + tn) / (tp + tn + fp + fn))

    return error, tpr, tnr, fpr, fnr


def roc_results(results, type_dict):
    y_true = []
    y_score = []

    for key in results:
        y_true.append(type_dict[key])
        y_score.append(results[key])

    roc_graph(*roc_curve(y_true, y_score))


def split_data(data, proportion, existing=[]):
    data = [item for item in data if item not in existing]
    result = np.random.choice(data, len(data) * proportion)
    return np.concatenate((result, existing))


def train_test_data(data_dict, training_proportion):
    training_data = np.random.choice(np.array(data_dict.keys()), len(data_dict) * training_proportion)

    train_data = {}
    test_data = {}

    for k, v in data_dict.iteritems():
        if k in training_data:
            train_data[k] = v
        else:
            test_data[k] = v

    return train_data, test_data


def get_ransomware_data():
    return get_data(
        'datasets/ransomware/AVnames.txt',
        'datasets/ransomware/matrix-antivirus-ransomware.txt',
        'datasets/ransomware/matrix-antivirus-goodware.txt'
    )


def get_old_large_data():
    return get_data(
        'datasets/old_large/engines.txt',
        'datasets/old_large/matrix-antivirus-malware.txt',
        'datasets/old_large/matrix-antivirus-goodware.txt'
    )

def get_new_large_data():
    return get_data(
        'datasets/new_large/engines.txt',
        'datasets/new_large/matrix-antivirus-malware.txt',
        'datasets/new_large/matrix-antivirus-goodware.txt'
    )

def get_newer_large_data():
    return get_data(
        'datasets/newer_large/engines.txt',
        'datasets/newer_large/matrix-antivirus-malware.txt',
        'datasets/newer_large/matrix-antivirus-goodware.txt'
    )


def get_data(engine_file, malware_file, goodware_file):
    engine_count = count_antivirus_engines(engine_file)
    ransomware_dict = create_data_dict(malware_file)
    goodware_dict = create_data_dict(goodware_file)

    data_dict = dict(ransomware_dict.items() + goodware_dict.items())

    type_dict = dict([(key, 1) for key in ransomware_dict] + [(key, 0) for key in goodware_dict])

    return engine_count, data_dict, type_dict

